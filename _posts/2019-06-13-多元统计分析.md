---

layout:     post
title:      多元统计分析
subtitle:   方法论总结
date:       2019-06-13
author:     Susie
header-img: img/post-bg-mountain.jpg
catalog: true
tags:
    - 统计
    - 期末
    - 总结
---



# 判别分析



# 聚类分析

## 系统聚类方法

### 基本思想
有n个样本，每个样本有m个指标，定义样本之间的距离和类和类之间的距离。每次合并距离最近的类，重新计算类间距离，直到最后合并所有类，将过程用遗传系谱图表达出来。
步骤：
1. 数据变换
2. 计算样本间的距离，得到距离矩阵
3. 初始状态，样本自己是一类
4. 对距离矩阵，合并类间距离最小的为一类
5. 计算新类到其他类的距离，得到新的距离矩阵
6. 画谱系图
7. 决定分类的个数和各类的成员

### 最短距离法
两个类之间距离最近的两个样本之间的距离

### 最长距离法
两个类之间相距最远的样本之间的距离

### 中间距离法
采用介于最短、最长之间的距离。
递推式:
$$D^2_{rk} = \frac{1}{2}(D_{pk}^2+D_{qk}^2)+\beta D^2_{pq}$$
通常取$$\beta=-\frac{1}{4}$，这样$D_{rk}$$就是以$$D_{qk},D_{pk},D_{pq}$$为边的三角形中$$D_{pq}$$边上的点。

### 重心法
计算的距离是类的重心之间的距离。$$n_p + n_q = n_r$$
$$\bar{X}^{(r)} =\frac{1}{n_r}(n_p\bar{X}^{(p)}+n_q\bar{X}^{(q)})$$
$$D_{rk}^2 = \frac{n_p}{n_r}D^2_{pk}+\frac{n_q}{n_r}D^2_{qk}-\frac{n_p}{n_r}\frac{n_q}{n_r}D^2_{pq}$$
### 类平均法
使用范围比较广泛，聚类效果比较好。用两类之间两两样本的平方距离的平均数作为衡量两个类的距离。
$$D_{rk}^2 = \frac{n_p}{n_r}D^2_{pk}+\frac{n_q}{n_r}D^2_{qk}$$

### 可变类平均法
$$\beta$$是＜1的参数，接近1的时候分类效果不好，一般取负数、
$$D_{rk}^2 = (1-\beta)[\frac{n_p}{n_r}D^2_{pk}+\frac{n_q}{n_r}D^2_{qk}]+\beta D^2_{pq}$$
### 可变法及MCQ相似分析法
$$D_{rk}^2 = \frac{(1-\beta)}{2}[\frac{n_p}{n_r}D^2_{pk}+\frac{n_q}{n_r}D^2_{qk}]+\beta D^2_{pq}$$
上面的是可变法，当$$\beta = 0$$的时候，是MCQ法(类平均法÷2)
### 离差平方和法（WARD法）

W是每个类当中的离差平方和。
$$D_{rk}^2 = \frac{n_p+n_k}{n_r+n_k}D^2_{pk}+\frac{n_q+n_k}{n_r+n_k}D^2_{qk}-\frac{n_k}{n_r+n_k} D^2_{pq}$$

这个应用比较广泛，但是规定了必须用欧式距离，不能用马氏距离或者Minkovski距离等。

## 类个数的确定
确定类个数的方法：

- 遗传谱系图
- 点的散布图
- 遗传谱系图的特点
- 统计量

### 统计量

1. $$R_k^2 = 1- \frac{SSA}{SST}$$ 值越大越好，说明分得开，总是随着k的增加而变大，可以通过画图来判断取第几个类。
2. 半偏$$R_k^2 = R_(k+1)^2 - R_k^2$$表示合并新类之后，类内离差平方和的增量。
3. 伪F统计量。$$F_k = \frac{B_k}{P_k}\cdot \frac{n-k}{k-1}$$
4. 伪T统计量$$t^2 = \frac{B_{KL}}{(W_K+W_L)/(n_K+n_L)}$$


# 主成分分析
把多个指标转化成少数几个综合指标，从而降维。
把指标进行线性变换，使线性变换后的方差尽可能大，尽可能反应全部指标的信息。如果一个（线性变换后的指标不够），就进行下一个线性变换，使主成分和第一个主成分之间正交，而且也尽可能反应更多的信息。这样，可以得到p个主成分，主成分方差大小递减。

主成分$$Z_i = a_i^\prime X$$需要满足的三个条件
1. 正则性：$$a_i^\prime a_i =1$$
2. 正交性：$$i>1,i\neq j, a_i^\prime \Sigma a_j = 0$$
3. 信息量：$$Var(Z_i) = \max Var(a\prime X)$$

几何意义：数据点散布在p维空间中，可以认为是椭球区域，通过变换坐标轴，找到波动性最大的方向，也即椭球的主轴。

主成分的性质：
1. $D(Z)=\Lambda$,A是正交矩阵，所以相当于把$$\Sigma$$对角化了
2. $\Sigma$对角线的标准差之和 = $$\lambda$$之和，可以用$$tr(\Sigma) = \Lambda$$证明
3. 主成分和原始变量之间的相关系数$$\rho_{ki} = \frac{\sqrt{\lambda_k}a_{ik}}{\sqrt{\sigma_{ii}}}$$
4. 相关系数平方和是1,相当于线性回归的R2

概念：主成分的贡献率
总体的贡献率是特征值/特征值和
对$X_i$的贡献率是相关系数平方和

# 因子分析

因子分析的应用：
1. 对样本进行分类
2. 探寻因子的内部结构

主成分分析和因子分析的区别
1. 【模型】主成分分析不需要数学模型，只是线性变换；因子分析需要构造构造因子模型（正交或斜交）
2. 【变量个数】主成分分析中主成分个数和变量个数相同，因子分析中要用尽可能少的公共因子
3. 【表示方法】主成分分析是把主成分表示成原始变量的线性组合，因子分析是将原始变量表示成公共因子和特殊因子的线性组合。

因子载荷：第j个因子预测第i个变量的回归系数

### 模型假设
1. 特殊因子间独立
2. 特殊因子和公共因子不相关
3. 公共因子互不相关

### 模型
$$X-\mu = AF + \epsilon$$

### 步骤
首要任务是用样本估计出$$\Sigma$$,然后用$$\Sigma - D = AA^\prime$$求得A和D

统计解释：
1. 因子载荷就是相关系数
2. **共同度**：$h_i^2$载荷阵矩阵行平方和
3. $$D(X_i) = h_i^2 +\sigma_i^2$$
4. **贡献**：列和，与公共因子有关，衡量公共因子相对重要性

### 估计方法
1. 主成分分析
2. 主因子法
3. 极大似然法

主成分： 谱分解，选最大的前几个特征值，$$A = (\sqrt{\lambda_1}l_1,\sqrt{\lambda_2}l_2,\cdots,\sqrt{\lambda_1}l_1)$$
A和主成分系数相差$$\sqrt{\lambda_j},\text{可以证明}\sum\sum\epsilon_{ij}^2 ≤\lambda_p^2+\cdots +\lambda_p^2$$

主因子：
如果已经知道特殊方差的初始估计,可以用$$h_i^2 = 1-\sigma^2$$代替相关阵R中的对角线元素。在对这个矩阵R进行特征值分解，过程和之前一样。

如何获得公因子方差$$h_i^2$$的初始估计？
1. 用第i个变量与其他所有变量的多重相关系数的平方，或者取$$\sigma_i^2 = 1/r^{ii}$$其中$$r^{ii}$$是R逆的对角元素
2. 取第i个变量与其他变量相关系数绝对值的最大值
3.  取1，等价于主成分解

极大似然：假设公共因子F和特殊因子$$\epsilon$$服从正态分布，就可以得到因子载荷阵和特殊方差的极大似然估计。
似然函数$$L(\bar{X},AA^\prime +D)$$是和A,D有关的函数，求A,D让它达到最大。可以增加$$A^\prime D^{-1}A$$是对角阵的约束条件，迭代可以求解。

### 因子得分
因子得分是每个样品的公共因子的估计，可以用于模型的诊断，也可以得到作进一步分析的原始数据。**不是参数估计**，而是对不可观测的随机变量F取值的估计。

- 最小二乘估计$$\hat{F} = (A^\prime A)^{-1}A^\prime X$$【通常和主成分法搭配使用】
- 加权最小二乘法，把误差方差的倒数作为权数。$$\hat{F} = (A^\prime D^{-1} A)^{-1}A^\prime D^{-1} X$$
- 【汤姆森因子得分】【回归法有偏】 $$\hat{F} = BX = A^\prime R^{-1} X$$
- 【贝叶斯 无偏】$$\hat{F} = A^\prime(AA^\prime +D)^{-1} X = A^\prime  \Sigma^{-1}X =  A^\prime  S^{-1}X$$

两种方法算出来的因子得分相似

# 对应分析

# 典型相关分析
